{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed8d836",
   "metadata": {},
   "source": [
    "## Step 1: Remove Useless columns\n",
    "In this step, we load all Shopify order files and combine them into a single DataFrame. We then remove columns that are either not useful for modeling (e.g., customer addresses or always-null fields) or columns that are filled in less than 5% of the rows. This results in a cleaner dataset with fewer missing values and less noise, which is essential for downstream modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = \"raw_data\"\n",
    "PROCESSED_DATA_PATH = \"processed_data\"\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"orders_cleaned_columns.csv\")\n",
    "\n",
    "# File paths\n",
    "order_files = [\n",
    "    os.path.join(RAW_DATA_PATH, \"shopify_orders\", \"orders_export_1.csv\"),\n",
    "    os.path.join(RAW_DATA_PATH, \"shopify_orders\", \"orders_export_2.csv\"),\n",
    "    os.path.join(RAW_DATA_PATH, \"shopify_orders\", \"orders_export_3.csv\"),\n",
    "]\n",
    "\n",
    "##########step 1 all orders in one dataframe##########\n",
    "# all orders tables in one dataframe\n",
    "orders_df = pd.concat([pd.read_csv(file) for file in order_files], ignore_index=True)\n",
    "\n",
    "\n",
    "################step 2 remove useless columns###############\n",
    "not_useful_columns = [\n",
    "    \"Currency\", # always EUR\n",
    "    \"Shipping\", # always 0\n",
    "    \"Shipping Method\", # always \"PostNL or nothing\"\n",
    "    \"Lineitem name\", # we already have an item sku\n",
    "    \"Lineitem compare at price\", # always empty\n",
    "    \"Lineitem requires shipping\", # always TRUE\n",
    "    \"Lineitem taxable\", # always TRUE\n",
    "    \"Lineitem fulfillment status\", # we have a column that indicates if the order is fulfilled or not \n",
    "    \"Billing Phone\", # (customer phone number) private customer information & does not benifit the model\n",
    "    \"Shipping Name\", # ( customer name ) private customer information & does not benifit the model\n",
    "    \"Shipping Street\", # ( customer address ) private customer information & does not benifit the model\n",
    "    \"Shipping Address1\", # ( customer address ) private customer information & does not benifit the model\n",
    "    \"Shipping Address2\", # ( customer address ) private customer information & does not benifit the model\n",
    "    \"Shipping Company\", # ( customer address ) private customer information & does not benifit the model\n",
    "    \"Shipping Zip\", # ( customer address ) private customer information & does not benifit the model\n",
    "    \"Shipping Phone\", # ( customer phone number ) private customer information & does not benifit the model\n",
    "    \"Notes\", # Payment notes are not useful for the model\n",
    "    \"Note Attributes\", # Payment notes are not useful for the model\n",
    "    \"Payment Reference\", # Payment notes are not useful for the model\n",
    "    \"Vendor\", # always Moodies Undies\n",
    "    \"Outstanding Balance\", # Does not benifit the model\n",
    "    \"Tax 1 Name\", # Does not benifit the model\n",
    "    \"Tax 1 Value\", # Does not benifit the model\n",
    "    \"Tax 2 Name\", # Does not benifit the model\n",
    "    \"Tax 2 Value\", # Does not benifit the model\n",
    "    \"Tax 3 Name\", # Does not benifit the model\n",
    "    \"Tax 3 Value\", # Does not benifit the model\n",
    "    \"Tax 4 Name\", # Does not benifit the model\n",
    "    \"Tax 4 Value\", # Does not benifit the model\n",
    "    \"Tax 5 Name\", # Does not benifit the model\n",
    "    \"Tax 5 Value\", # Does not benifit the model\n",
    "    \"Phone\", # ( customer phone number ) private customer information & does not benifit the model\n",
    "    \"Receipt Number\", # Does not benifit the model\n",
    "    \"Payment Terms Name\",\n",
    "    \"Next Payment Due At\",\n",
    "    \"Payment References\"\n",
    "]\n",
    "\n",
    "orders_df.drop(columns=not_useful_columns, inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "orders_df.to_csv(output_path, index=False)\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e168e1d",
   "metadata": {},
   "source": [
    "## Step 2: Combine Multiple Rows Into One Per Order\n",
    "Shopify splits a single order into multiple rows if it contains multiple unique items. This step identifies such rows using the 'Email' and 'Name' (which is the id of an order) fields, and merges them by combining their line items into a single list. Additionally, duplicate orders are filtered to keep only the fulfilled ones. The result is a dataset where each order occupies one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_file = os.path.join(PROCESSED_DATA_PATH, \"orders_cleaned_columns.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"orders_have_one_row.csv\")\n",
    "orders_df = pd.read_csv(orders_file)\n",
    "\n",
    "orders_df[\"Lineitems\"] = orders_df.apply(\n",
    "    lambda row: [{row[\"Lineitem sku\"] if pd.notna(row[\"Lineitem sku\"]) else \"unknown\": row[\"Lineitem quantity\"]}],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rows_to_drop = []\n",
    "for i in range(1, len(orders_df)):\n",
    "    current_row = orders_df.loc[i]\n",
    "    previous_row = orders_df.loc[i - 1]\n",
    "    if pd.isna(current_row[\"Paid at\"]) and current_row[\"Name\"] == previous_row[\"Name\"] and current_row[\"Email\"] == previous_row[\"Email\"]:\n",
    "        prev_items = previous_row[\"Lineitems\"]\n",
    "        curr_items = current_row[\"Lineitems\"]\n",
    "        prev_items.extend(curr_items)\n",
    "        orders_df.at[i - 1, \"Lineitems\"] = prev_items\n",
    "        rows_to_drop.append(i)\n",
    "\n",
    "orders_df.drop(index=rows_to_drop, inplace=True)\n",
    "orders_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicated orders unless fulfilled\n",
    "duplicate_names = orders_df[\"Name\"][orders_df[\"Name\"].duplicated(keep=False)]\n",
    "filtered_orders_df = orders_df[\n",
    "    ~orders_df[\"Name\"].isin(duplicate_names) |\n",
    "    (orders_df[\"Fulfillment Status\"] == \"fulfilled\")\n",
    "]\n",
    "filtered_orders_df = filtered_orders_df.drop_duplicates(subset=\"Name\", keep=\"first\")\n",
    "orders_df = filtered_orders_df.reset_index(drop=True)\n",
    "\n",
    "orders_df.to_csv(output_path, index=False)\n",
    "print(f\"Updated file saved to: {output_path}\")\n",
    "orders_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa442df",
   "metadata": {},
   "source": [
    "## Step 3: Merge Klaviyo Customer Data\n",
    "We merge the cleaned Shopify order data with customer profile data from Klaviyo using the 'Email' field. This ensures that we capture both customers who have made purchases and those who have not. The result is a dataset that combines behavioral (order-based) and demographic (profile-based) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_file = os.path.join(PROCESSED_DATA_PATH, \"orders_have_one_row.csv\")\n",
    "klaviyo_file = os.path.join(RAW_DATA_PATH, \"Klaviyo_everyone_email.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"orders_with_customer_data.csv\")\n",
    "\n",
    "orders_df = pd.read_csv(orders_file)\n",
    "customers_df = pd.read_csv(klaviyo_file)\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    customers_df,\n",
    "    orders_df,\n",
    "    on=\"Email\",\n",
    "    how=\"outer\",\n",
    "    suffixes=(\"\", \"_order\")\n",
    ")\n",
    "\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged file with all customers saved to: {output_path}\")\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffb706",
   "metadata": {},
   "source": [
    "## Step 4: Remove columns that are filled for in less than 5 percent of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"orders_with_customer_data.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"orders_with_customer_data_no_empty_columns.csv\")\n",
    "\n",
    "def get_fill_percentages(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary with the percentage of filled (non-null and non-empty) rows per column.\n",
    "\n",
    "    :param df: DataFrame to inspect.\n",
    "    :return: Dictionary {column_name: fill_percentage}\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    fill_percentages = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        non_empty = df[col].notna() & (df[col].astype(str).str.strip() != \"\")\n",
    "        fill_percentage = (non_empty.sum() / total_rows) * 100\n",
    "        fill_percentages[col] = round(fill_percentage, 2)\n",
    "\n",
    "    return fill_percentages\n",
    "\n",
    "\n",
    "orders_with_customer_df = pd.read_csv(input_path)\n",
    "fill_stats = get_fill_percentages(orders_with_customer_df)\n",
    "\n",
    "for col, pct in fill_stats.items():\n",
    "    print(f\"{col}: {pct}% filled\")\n",
    "    if pct < 5:\n",
    "        print(f\"{col} has a fill rate of {pct}% which is below the threshold of 95%. column {col} will be dropped.\")\n",
    "        orders_df.drop(columns=[col], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd928e1",
   "metadata": {},
   "source": [
    "## Step 5: Create Monthly Spend and Order Features\n",
    "Here, we engineer new features for modeling. For each customer, we compute the number of orders and the amount spent in each month. These features are crucial for understanding customer behavior over time and are especially useful for predicting future value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_file = os.path.join(PROCESSED_DATA_PATH, \"orders_have_one_row.csv\")\n",
    "output_file = os.path.join(PROCESSED_DATA_PATH, \"monthly_customer_stats.csv\")\n",
    "\n",
    "orders_df = pd.read_csv(orders_file, low_memory=False)\n",
    "orders_df[\"Paid at\"] = pd.to_datetime(orders_df[\"Paid at\"], utc=True, errors=\"coerce\")\n",
    "orders_df = orders_df.dropna(subset=[\"Paid at\"])\n",
    "orders_df[\"YearMonth\"] = orders_df[\"Paid at\"].dt.strftime(\"%Y_%m\")\n",
    "\n",
    "monthly_summary = orders_df.groupby([\"Email\", \"YearMonth\"]).agg(\n",
    "    orders_per_month=(\"Name\", \"count\"),\n",
    "    amount_per_month=(\"Total\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "order_counts = monthly_summary.pivot(index=\"Email\", columns=\"YearMonth\", values=\"orders_per_month\")\n",
    "amounts_spent = monthly_summary.pivot(index=\"Email\", columns=\"YearMonth\", values=\"amount_per_month\")\n",
    "\n",
    "order_counts = order_counts.reindex(sorted(order_counts.columns), axis=1)\n",
    "amounts_spent = amounts_spent.reindex(sorted(amounts_spent.columns), axis=1)\n",
    "\n",
    "order_counts.columns = [f\"orders_in_{col}\" for col in order_counts.columns]\n",
    "amounts_spent.columns = [f\"amount_spent_{col}\" for col in amounts_spent.columns]\n",
    "\n",
    "monthly_stats = pd.concat([order_counts, amounts_spent], axis=1)\n",
    "monthly_stats[\"total_amount_spent\"] = orders_df.groupby(\"Email\")[\"Total\"].sum()\n",
    "\n",
    "monthly_stats = monthly_stats.reset_index()\n",
    "monthly_stats.to_csv(output_file, index=False)\n",
    "print(f\"Monthly customer stats saved to: {output_file}\")\n",
    "monthly_stats.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ec258",
   "metadata": {},
   "source": [
    "## Step 5: Merge Monthly Features Into Full Dataset\n",
    "In this final preprocessing step, we merge the engineered monthly features into the combined Klaviyo + Shopify dataset. This gives us one complete table that includes customer profile data, order history, and time-based behavioral features, ready for training a Customer Lifetime Value prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_customers_file = os.path.join(PROCESSED_DATA_PATH, \"orders_with_customer_data.csv\")\n",
    "monthly_stats_file = os.path.join(PROCESSED_DATA_PATH, \"monthly_customer_stats.csv\")\n",
    "output_file = os.path.join(PROCESSED_DATA_PATH, \"final_merged_data.csv\")\n",
    "\n",
    "orders_customers_df = pd.read_csv(orders_customers_file)\n",
    "monthly_stats_df = pd.read_csv(monthly_stats_file)\n",
    "\n",
    "final_df = pd.merge(\n",
    "    orders_customers_df,\n",
    "    monthly_stats_df,\n",
    "    on=\"Email\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"Final merged dataset saved to: {output_file}\")\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c7d38",
   "metadata": {},
   "source": [
    "### Step 6 â€“ Final Feature Selection and Aggregation\n",
    "\n",
    "In this step, we finalized our customer-level dataset by cleaning, transforming, and aggregating features to prepare them for modeling.\n",
    "\n",
    "#### Column Filtering\n",
    "We removed over 70 columns that were either:\n",
    "- Sensitive (e.g. names, addresses, phone numbers)\n",
    "- Duplicated or irrelevant for modeling\n",
    "- High-cardinality categorical columns unlikely to provide predictive power\n",
    "- System-generated metadata or unused campaign tracking codes\n",
    "\n",
    "#### Boolean Normalization\n",
    "We identified object-type columns that semantically represented booleans (e.g., `InWelcomeFlow`, `InCheckoutAbandonmentFlow`, `Accepts Marketing`) and explicitly converted them to `True`/`False`, filling missing values as `False`.\n",
    "\n",
    "#### Location Cleanup\n",
    "Missing values in the `Country` column were assumed to be â€œNetherlandsâ€, as that is the primary market. Missing `City`, `Last Source`, and `Campaign` fields were filled with `\"unknown\"` or `\"(direct)\"` where applicable.\n",
    "\n",
    "#### Date Feature Engineering\n",
    "We engineered customer timeline features based on:\n",
    "- First and last activity\n",
    "- First and last purchase\n",
    "- Days since email marketing consent\n",
    "\n",
    "This resulted in numerical columns like `days_since_first_active` and `customer_lifetime_days`, giving the model a richer temporal understanding of each customer.\n",
    "\n",
    "#### Email Marketing Consent\n",
    "Instead of treating `Email Marketing Consent Timestamp` as a raw date, we extracted:\n",
    "- `email_marketing_optin`: binary indicator if consent was ever given\n",
    "- `days_since_email_optin`: numeric time since consent (or -1 if not given)\n",
    "\n",
    "This allowed us to convert a sparse timestamp into usable model features.\n",
    "\n",
    "#### Lineitem Breakdown\n",
    "Each row in the dataset originally represented a customer order and contained a `Lineitems` field â€” a list of `{sku: quantity}` dictionaries.\n",
    "We:\n",
    "- Counted total quantities (`total_amount_line_items_bought`)\n",
    "- Extracted the first and second unique SKUs bought, plus their quantities (`first_item_sku`, `first_item_qty`, etc.)\n",
    "\n",
    "This allowed us to preserve product-level information in a compact way, without introducing excessive sparsity.\n",
    "\n",
    "####  Aggregation to Customer Level\n",
    "As multiple orders could exist for a single customer, we grouped the dataset by `Email`, and:\n",
    "- Summed all numeric fields (e.g. amount spent, quantity, days)\n",
    "- Took the first occurrence of remaining non-numeric fields\n",
    "\n",
    "This gave us one row per customer, suitable for training a model to predict their future value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f81956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "input_file = os.path.join(\"processed_data\", \"final_merged_data.csv\")\n",
    "output_file = os.path.join(\"processed_data\", \"clv_model_data.csv\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Drop irrelevant or sensitive columns\n",
    "columns_to_drop = [\n",
    "    'First Name', 'Last Name', 'Phone Number', 'Address', 'Address 2', 'Locale',\n",
    "    'Locale: Country', 'Organization', 'Title', 'Birthday', 'birthday', '$birthday_source',\n",
    "    'Billing Name', 'Billing Street', 'Billing Address1', 'Billing Address2', 'Billing Company',\n",
    "    'Billing City', 'Billing Zip', 'Billing Province', 'Billing Country', 'Billing Province Name',\n",
    "    'Shipping City', 'Shipping Province', 'Shipping Country', 'Shipping Province Name',\n",
    "    'Email Suppressions', 'Email Suppressions Timestamp',\n",
    "    'Email List Suppressions', 'Email List Suppressions Timestamp', 'Email List Suppressions Reasons',\n",
    "    'Initial Referring Domain', 'Search Keyword', 'Initial Source Referrer',\n",
    "    'Created at', 'Cancelled at', 'Payment Method', 'Payment ID', 'Id', 'Tags', 'Risk Level', 'Source',\n",
    "    'Subtotal', 'Taxes', 'Total', 'Discount Code', 'Refunded Amount', 'Duties',\n",
    "    'Lineitem quantity', 'Lineitem price', 'Lineitem sku', 'Lineitem discount',\n",
    "    'Paid at', 'Fulfilled at', 'Financial Status', 'Accepts Marketing_order', 'undefined',\n",
    "    'Profile Created On', 'Date Added', 'Last Open', 'Last Click',\n",
    "    'Device ID', 'Location', 'Employee',\n",
    "    'SMS Marketing Consent', 'SMS Marketing Consent Timestamp',\n",
    "    'SMS Transactional Consent', 'SMS Transactional Consent Timestamp',\n",
    "    'Total Customer Lifetime Value', 'Historic Customer Lifetime Value',\n",
    "    'Historic Number Of Orders', 'Initial Source Content', 'Last Source Term',\n",
    "    'Last Source First Page', 'Last Source Medium', 'Last Source Content', \n",
    "    '$timezone', 'CollectionPreference',\n",
    "    'Fulfillment Status', 'Name', 'ZPD: Situation', 'VIP', 'UTM Source',\n",
    "    'UTM Medium', 'ResendFulfilledOrderFlow', 'Survey 01/23',\n",
    "    'T-LwHoyq: Swimwear: yes', 'UTM Campaign', 'UTM Content',\n",
    "    'Last Source Referrer', 'Initial Source First Page', 'Initial Source Term',\n",
    "    'Expected Date Of Next Order'\n",
    "]\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Convert object-type booleans\n",
    "bool_like_cols = ['InCheckoutAbandonmentFlow', 'InFulfilledOrderFlow', 'InWelcomeFlow', 'Accepts Marketing']\n",
    "for col in bool_like_cols:\n",
    "    df[col] = df[col].fillna(False).astype(bool)\n",
    "\n",
    "# Fill defaults for important string fields\n",
    "df[\"Country\"] = df[\"Country\"].fillna(\"Netherlands\")\n",
    "df[\"City\"] = df[\"City\"].fillna(\"unknown\")\n",
    "df[\"Last Source Campaign\"] = df[\"Last Source Campaign\"].fillna(\"unknown\")\n",
    "df[\"Last Source\"] = df[\"Last Source\"].fillna(\"unknown\")\n",
    "df[\"Initial Source Campaign\"] = df[\"Initial Source Campaign\"].fillna(\"(direct)\")\n",
    "\n",
    "# Parse Lineitems\n",
    "df[\"Lineitems\"] = df[\"Lineitems\"].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Convert dates\n",
    "date_cols = [\"First Active\", \"Last Active\", \"First Purchase Date\", \"Last Purchase Date\"]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "reference_date = datetime(2025, 4, 1)\n",
    "\n",
    "# Feature engineering from dates\n",
    "df[\"email_marketing_optin\"] = df[\"Email Marketing Consent Timestamp\"].notna().astype(int)\n",
    "df[\"days_since_email_optin\"] = (\n",
    "    datetime(2025, 4, 1) - pd.to_datetime(df[\"Email Marketing Consent Timestamp\"], errors=\"coerce\")\n",
    ").dt.days\n",
    "\n",
    "# Fill missing with 0 or -1 depending on what makes more sense\n",
    "df[\"days_since_email_optin\"] = df[\"days_since_email_optin\"].fillna(-1)\n",
    "df.drop(columns=[\"Email Marketing Consent Timestamp\"], inplace=True)\n",
    "\n",
    "df[\"days_since_first_active\"] = (reference_date - df[\"First Active\"]).dt.days\n",
    "df[\"days_since_last_active\"] = (reference_date - df[\"Last Active\"]).dt.days\n",
    "df[\"days_since_first_purchase\"] = (reference_date - df[\"First Purchase Date\"]).dt.days\n",
    "df[\"days_since_last_purchase\"] = (reference_date - df[\"Last Purchase Date\"]).dt.days\n",
    "df[\"customer_lifetime_days\"] = (df[\"Last Purchase Date\"] - df[\"First Purchase Date\"]).dt.days\n",
    "\n",
    "# Drop original datetime columns\n",
    "df.drop(columns=date_cols, inplace=True)\n",
    "\n",
    "# Count total quantity of items in Lineitems\n",
    "df[\"order_items_count\"] = df[\"Lineitems\"].apply(\n",
    "    lambda items: sum(qty for item in items for qty in item.values()) if isinstance(items, list) else 0\n",
    ")\n",
    "\n",
    "# Aggregate to one row per customer\n",
    "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "grouped_df = df.groupby(\"Email\").agg({\n",
    "    **{col: 'sum' for col in numeric_cols},\n",
    "    **{col: 'first' for col in df.columns if col not in numeric_cols and col != \"Email\"}\n",
    "}).reset_index()\n",
    "\n",
    "# Rename engineered columns\n",
    "grouped_df.rename(columns={\n",
    "    \"Discount Amount\": \"total_discount_amount\",\n",
    "    \"order_items_count\": \"total_amount_line_items_bought\"\n",
    "}, inplace=True)\n",
    "\n",
    "# âœ¨ Extract first, second, third item SKU and qty\n",
    "def extract_item_and_qty(items, index):\n",
    "    if isinstance(items, list) and len(items) > index:\n",
    "        item = list(items[index].items())[0]\n",
    "        return item[0], item[1]\n",
    "    return None, 0\n",
    "\n",
    "for i, label in enumerate([\"first\", \"second\"]):\n",
    "    grouped_df[f\"{label}_item_sku\"] = grouped_df[\"Lineitems\"].apply(lambda items: extract_item_and_qty(items, i)[0])\n",
    "    grouped_df[f\"{label}_item_qty\"] = grouped_df[\"Lineitems\"].apply(lambda items: extract_item_and_qty(items, i)[1])\n",
    "\n",
    "# Save final dataset\n",
    "grouped_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved cleaned customer-level dataset to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbaab7b",
   "metadata": {},
   "source": [
    "## Step 7: Create the Target Variable (CLV_next_3_months)\n",
    "\n",
    "To train a model that predicts customer lifetime value (CLV) for the next 3 months, we first need to define our target.\n",
    "\n",
    "### What weâ€™re predicting:\n",
    "We are summing the total amount a customer spent in the months of January, February, and March 2025. This becomes the column `CLV_next_3_months`, which will serve as our prediction target.\n",
    "\n",
    "### Why this approach:\n",
    "- Simple and interpretable\n",
    "- Allows the model to focus on short-term CLV\n",
    "- Aligns with business needs like quarterly revenue forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = os.path.join(\"processed_data\", \"clv_model_data.csv\")\n",
    "output_file = os.path.join(\"processed_data\", \"clv_model_data_with_target.csv\")\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Define the target variable\n",
    "target_months = [\"amount_spent_2025_01\", \"amount_spent_2025_02\", \"amount_spent_2025_03\"]\n",
    "df[\"CLV_next_3_months\"] = df[target_months].sum(axis=1)\n",
    "\n",
    "# Drop future columns from features to avoid leakage\n",
    "df.drop(columns=target_months, inplace=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"Target variable created and future columns dropped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d9cd2",
   "metadata": {},
   "source": [
    "## Step 8: Preprocess Features for Modeling\n",
    "\n",
    "Before training a machine learning model, we need to prepare our feature set.\n",
    "\n",
    "### What we do:\n",
    "- Drop identifier columns like `Email` which are not predictive\n",
    "- Convert boolean columns into integer format (`True` â†’ `1`, `False` â†’ `0`)\n",
    "- One-hot encode object/categorical columns for compatibility with XGBoost\n",
    "- Replace special characters in column names to avoid parsing errors\n",
    "- Fill remaining missing values with `0` to avoid training issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with target\n",
    "input_file = os.path.join(\"processed_data\", \"clv_model_data_with_target.csv\")\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Drop identifiers\n",
    "df = df.drop(columns=[\"Email\"])\n",
    "\n",
    "# Encode boolean columns as integers\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# Fill NaNs\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"CLV_next_3_months\"])\n",
    "y = df[\"CLV_next_3_months\"].squeeze()\n",
    "\n",
    "# Encode object/categorical columns using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Replace ALL non-alphanumeric characters with underscores\n",
    "X.columns = X.columns.str.replace(r\"[^a-zA-Z0-9_]\", \"_\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936332b",
   "metadata": {},
   "source": [
    "## Step 9: Train/Test Split\n",
    "\n",
    "To evaluate how well our model generalizes to unseen customers, we split our dataset into training and testing sets.\n",
    "\n",
    "We use an 80/20 split, a common practice in supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = np.asarray(y_train).ravel()  # 1D target\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = np.asarray(y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e094070",
   "metadata": {},
   "source": [
    "## Step 10: Baseline for CLV Prediction\n",
    "Zero Spending: Predict 0 for all customers.\n",
    "\n",
    "Global Average: Predict average of 3-month spend across all customers.\n",
    "\n",
    "Customer-Specific Monthly Average Ã— 3: Predict Total Amount Spent / # active months Ã— 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6184c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# True values\n",
    "y_true = y_test_np\n",
    "\n",
    "# 1. Zero spending baseline\n",
    "y_zero = np.zeros_like(y_true)\n",
    "\n",
    "# 2. Global average baseline\n",
    "global_avg = y_train_np.mean()\n",
    "y_global = np.full_like(y_true, global_avg)\n",
    "\n",
    "# 3. Customer-specific monthly avg Ã— 3 (only if available)\n",
    "# For now, we simulate a per-customer average from training set\n",
    "# You can replace this logic with actual per-customer monthly avg if available\n",
    "y_personal_avg = np.full_like(y_true, (y_train_np.mean() / 3) * 3)\n",
    "\n",
    "# Define a helper function\n",
    "def print_metrics(name, y_pred):\n",
    "    print(f\"ðŸ”¹ {name}\")\n",
    "    print(\"  RMSE:\", round(mean_squared_error(y_true, y_pred), 2))\n",
    "    print(\"  MAE :\", round(mean_absolute_error(y_true, y_pred), 2))\n",
    "    print(\"  RÂ²  :\", round(r2_score(y_true, y_pred), 4))\n",
    "\n",
    "# Show results\n",
    "print_metrics(\"Zero Spending Baseline\", y_zero)\n",
    "print_metrics(\"Global Average Baseline\", y_global)\n",
    "print_metrics(\"Customer Avg x 3 Baseline\", y_personal_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a61dc7",
   "metadata": {},
   "source": [
    "## Step 10: Choose, Train, and Evaluate Model\n",
    "\n",
    "We will use **XGBoost Regressor**, a powerful and efficient model for tabular data.\n",
    "\n",
    "### Why XGBoost:\n",
    "- Performs well on structured data\n",
    "- Handles missing values internally\n",
    "- Provides feature importance insights\n",
    "- Scales well for large datasets\n",
    "\n",
    "We will train the model and evaluate its performance using:\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- RÂ² Score (explained variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model = XGBRegressor(random_state=42)\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_np)\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test_np, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test_np, y_pred))\n",
    "print(\"RÂ² Score:\", r2_score(y_test_np, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14114b9d",
   "metadata": {},
   "source": [
    "Great the model gets just above 85% accuracy.\n",
    "## let's plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model performance metrics\n",
    "rmse = 22.36\n",
    "mae = 2.12\n",
    "r2 = 0.8547\n",
    "\n",
    "# Labels and values\n",
    "metrics = ['RMSE', 'MAE', 'RÂ² Score']\n",
    "values = [rmse, mae, r2]\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics, values, color='skyblue')\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, max(values) + 5)\n",
    "\n",
    "# Annotate values on bars\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Predicted vs Actual Scatter Plot\n",
    "# Shows how well predictions align with true values.\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_np, y_pred, alpha=0.3)\n",
    "plt.plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], 'r--')\n",
    "plt.xlabel(\"Actual CLV\")\n",
    "plt.ylabel(\"Predicted CLV\")\n",
    "plt.title(\"Predicted vs Actual CLV\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
